- hosts: localhost
  connection: local
  gather_facts: no
  tasks:
    - name: "Destroy Azure Deploy  Resource group {{ resource_group_name }}"
      azure_rm_deployment:
        state: absent
        subscription_id: "{{ subscriptionID }}"
        resource_group_name: "{{ resource_group_name }}"
        location: "{{ region }}"
################################################
####  Create basic infra components
################################################
    - name: "Create Azure Deploy {{ resource_group_name }}"
      azure_rm_deployment:
        state: present
        subscription_id: "{{ subscriptionID }}"
        resource_group_name: "{{ resource_group_name }}"
        template_link: 'https://raw.githubusercontent.com/ivanthelad/ansible-azure/master/infra.json'
        location: "{{ region }}" 
      register: azure_vm
      
      
################################################
####  Create basic humphost node, this exposes 8443 and 22 as a public ports on a public ip , in future this will be a jump host and also entry point for the masters 
###   we use this node to jump into the other nodes
################################################
    - name: Create Jumphost Deployment
      with_dict: "{{ jumphost }}"
      azure_rm_deployment:
        deployment_mode: incremental
        state: present
        subscription_id: "{{ subscriptionID }}"
        resource_group_name: "{{ resource_group_name }}"
        location: "{{ region }}" 
        #### We add it to the master subnet but really don't matter 
        deployment_name: "{{ item.value.name }}"
        parameters:
          image:
            value: rhel
          subnetName:
            value: masterSubnet
          virtualNetworkName:
            value: openshiftVnet
          dnsName:
            value: "master-{{ resource_group_name }}"
          adminUsername:
            value: "{{ adminUsername }}"
          vmSize:
            value: "Standard_D1"
          adminPassword:
            value: "{{ adminPassword }}"
          vmName:
            value:  "{{ item.value.name }}"
          tags:
            value: "{{ item.value.tags }}"
          sshKeyData:
            value: "{{ sshkey }}"
        template_link: 'https://raw.githubusercontent.com/ivanthelad/ansible-azure/master/master.json'
      register: azure_async
      async: 7200
      poll: 0
    - name: Wait for jump instance creation to complete
      async_status: jid={{ item.ansible_job_id }}
      register: azure
      until: azure.finished
      retries: 320
      with_items: "{{ azure_async.results }}"


    - debug: msg="the value of azure  {{ azure }}"
    - debug: msg="the value of azure_async {{ azure_async }}"

    - name: Add new instance to host group
      add_host: hostname={{ item['ips'][0].public_ip }} groupname=azure_vms
      with_items: azure.results[0].deployment.instances
    - set_fact:
        publicjumpdns: "{{ azure.results[0].deployment.instances[0].ips[0].dns_settings.fqdn }}"
        publicjumpip: "{{ azure.results[0].deployment.instances[0].ips[0].public_ip }}"
        lbvmname: "{{ azure.results[0].deployment.instances[0].vm_name }}"

    - debug: msg="the value of azure  {{ azure }}"
    - debug: msg="the value of dns  {{ publicjumpdns }}"
    - debug: msg="the value of ip  {{ publicjumpip }}"
      ################################################
      ####  Only setting these values so we can manually look up. bit of a hack
      ################################################
    - lineinfile:
        dest: group_vars/all
        regexp: "^publicjumpdns:"
        line: "publicjumpdns: {{ publicjumpdns }}"
    - lineinfile:
        dest: group_vars/all
        regexp: "^publicjumpip:"
        line: "publicjumpip: {{ publicjumpip }}"
    

################################################
####  Adding the jump host to our local know hosts
################################################
- hosts: localhost
  connection: local
  tasks:
    - name: Scan the public key for master and added it to known hosts
      shell: "ssh-keyscan -H -T 10  {{ item }} >> ~/.ssh/known_hosts "
      with_items: groups['azure_vms']



################################################
#### Generating key on jump host. This then download locally to /tmp//id_rsa.tmp and shared to other newly create azure hosts on initialization
#### Rather complicated way but it avoids opening port 22 on all hosts
################################################
- hosts: azure_vms
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"

  user: "{{ adminUsername }}"
  tasks:
    - name: Wait for SSH to come up
      wait_for: port=22 timeout=2000 state=started
    - name: echo the hostname of the vm
      shell: hostname
    - name: Generating RSA key for root
      user: name={{ adminUsername }} generate_ssh_key=yes
    - name: Downloading pub key
      fetch: src=/home/{{ adminUsername }}/.ssh/id_rsa.pub dest=/tmp/id_rsa.tmp flat=yes
    - authorized_key: user="{{ adminUsername }}" key="{{ lookup('file', '/tmp/id_rsa.tmp') }}"


################################################
#### Subscribe the jump hosts. need to install bits like ansible and "atomic-openshift-utils"
################################################
- hosts: azure_vms
  user: "{{ adminUsername }}"
  sudo: true
  roles:
     - { role: subscribe, rhnuser: "{{ rh_subcription_user }}", rhnpassword: "{{ rh_subcription_pass }}" }
  tasks:
    - name: Install Basic Preqes
      yum:
        name: "{{ item }}"
        state: present
      with_items:
           - git
           - vim
           - ansible
           - atomic-openshift-utils


################################################
#### because
################################################
- hosts: azure_vms
  user: "{{ adminUsername }}"
  vars:
    contents: "{{ lookup('file', '/tmp/id_rsa.tmp') }}"
  tasks:
    - debug: msg="the value of foo.txt is {{ contents }}"
### infra here


################################################
####  Create basic Master node, this exposes 8443 and 22 as a public ports on a public ip , in future this will be a jump host
###   we use this node to jump into the other nodes
################################################
################################################
#### Creating the masters nodes. these sit behind the jump host. no need to open public ports
####Todo, dyncamically add the list of infra nodes to group
################################################
- hosts: localhost
  strategy: free
  serial: 3
  connection: local
  vars:
      masterkey: "{{ lookup('file', '/tmp/id_rsa.tmp') }}"
  gather_facts: no
  tasks:
    - name: Create Masters Deploy
      async: 7200
      poll: 0
      with_dict: "{{ masters }}"
      azure_rm_deployment:
        deployment_mode: incremental
        state: present
        subscription_id: "{{ subscriptionID }}"
        resource_group_name: "{{ resource_group_name }}"
        location: "{{ region }}"
        deployment_name: "{{ item.value.name }}"
        parameters:
          subnetName:
            value: masterSubnet
          virtualNetworkName:
            value: openshiftVnet
          adminUsername:
            value: "{{ adminUsername }}"
          vmSize:
            value: "{{ node_vmSize }}"
          adminPassword:
            value: "{{ adminPassword }}"
          image:
            value: rhel
          masterKey:
            value: "{{ masterkey }}"
          tags:
            value: "{{ item.value.tags }}"
          vmName:
            value: "{{ item.value.name }}"
          sshKeyData:
            value: "{{ sshkey }}"
        template_link: 'https://raw.githubusercontent.com/ivanthelad/ansible-azure/master/nodes.json'
        #i#template_link: 'ansible-azure/nodes.json'
      register: azuremasternodes_sync
    - name: Wait for master instance creation to complete
      async_status: jid={{ item.ansible_job_id }}
      register: azuremasternodes
      until: azuremasternodes.finished
      retries: 900
      with_items: "{{ azuremasternodes_sync.results }}"

################################################
#### Creating the infrastrucute nodes.
####Todo, dyncamically add the list of infra nodes to group
################################################
- hosts: localhost
#i#  serial: 2
  strategy: free
  connection: local
  vars:
      masterkey: "{{ lookup('file', '/tmp/id_rsa.tmp') }}"
  gather_facts: no
  tasks:
    - name: Create infra Nodes Deploy
      with_dict: "{{ infranodes }}"
      async: 7200
      poll: 0
      azure_rm_deployment:
        deployment_mode: incremental
        state: present
        subscription_id: "{{ subscriptionID }}"
        resource_group_name: "{{ resource_group_name }}"
        location: "{{ region }}" 
        deployment_name: "{{ item.value.name }}"
        parameters:
          subnetName:
            value: infranodeSubnet
          virtualNetworkName:
            value: openshiftVnet
          adminUsername:
            value: "{{ adminUsername }}"
          vmSize:
            value: "{{ node_vmSize }}"
          adminPassword:
            value: "{{ adminPassword }}"
          image:
            value: rhel
          masterKey:
            value: "{{ masterkey }}"
          tags:
            value: "{{ item.value.tags }}"
          vmName:
            value: "{{ item.value.name }}"
          sshKeyData:
            value: "{{ sshkey }}"
        template_link: 'https://raw.githubusercontent.com/ivanthelad/ansible-azure/master/infranode.json'
        #i#template_link: 'ansible-azure/nodes.json'
      register: azureinfranodes_async
    - name: Wait for infranode instance creation to complete
      async_status: jid={{ item.ansible_job_id }}
      register: azureinfranodes
      until: azureinfranodes.finished
      retries: 900
      with_items: "{{ azureinfranodes_async.results }}"
    - set_fact:
        routerpublicip: "{{ azureinfranodes.results[0].deployment.instances[0].ips[0].public_ip }}"
    - debug: msg="the value of azure  {{ azure }}"
    - debug: msg="the value of ip  {{ routerpublicip }}"

################################################
#### Creating the application nodes.
####Todo, dyncamically add the list of infra nodes to group
################################################
- hosts: localhost
  serial: 3
  strategy: free
  connection: local
  vars:
      masterkey: "{{ lookup('file', '/tmp/id_rsa.tmp') }}"
  gather_facts: no
  tasks:
    - name: Create Nodes Deploy
      async: 7200
      poll: 0
      with_dict: "{{ nodes }}"
      azure_rm_deployment:
        deployment_mode: incremental
        state: present
        subscription_id: "{{ subscriptionID }}"
        resource_group_name: "{{ resource_group_name }}"
        location: "{{ region }}" 
        deployment_name: "{{ item.value.name }}"
        parameters:
          subnetName:
            value: nodeSubnet
          virtualNetworkName:
            value: openshiftVnet
          adminUsername:
            value: "{{ adminUsername }}"
          vmSize:
            value: "{{ node_vmSize }}"
          adminPassword:
            value: "{{ adminPassword }}"
          image:
            value: rhel
          masterKey:
            value: "{{ masterkey }}"
          tags:
            value: "{{ item.value.tags }}"
          vmName:
            value: "{{ item.value.name }}"
          sshKeyData:
            value: "{{ sshkey }}"
        template_link: 'https://raw.githubusercontent.com/ivanthelad/ansible-azure/master/nodes.json'
        #i#template_link: 'ansible-azure/nodes.json'
      register: azurenodes_async
    - name: Wait for node instance creation to complete
      async_status: jid={{ item.ansible_job_id }}
      register: azurenodes
      until: azurenodes.finished
      retries: 900
      with_items: " {{ azurenodes_async.results }}"

################################################
#### This is kinda messy, While you can technially jump onto the jump machine and  call ansible playbook
####    ---  /bin/ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
####  In this case i'm using to jump host as a 'gatewayed' ansible proxy. basically proxying my ansible commands via the jump host.
#### To do this seamlessly the local machine requires the hosts to be added to know hosts
#### This the funky stuff that going on below... ohh forgive me
####
################################################
- hosts: azure_vms
  user: "{{ adminUsername }}"
  tasks:
## From the jumphost scan the keys for all app nodes  and print to file
    - name: From the jumphost scan the keys for all app nodes  and print to file
      shell: "ssh-keyscan -H -T 10  {{ item.value.name }} >> /home/{{ adminUsername }}/.ssh/known_hosts "
      with_dict: "{{ nodes }}"
## From the jumphost fetch the keys for all app nodes  and save locally
    - name: From the jumphost fetch the keys for all app nodes  and save locally
      fetch: src=/home/{{ adminUsername }}/.ssh/known_hosts  dest=/tmp/{{ item.value.name }}_knownhosts flat=yes
      with_dict: "{{ nodes }}"

## From the jumphost scan the keys for all infranodes  and print to file
    - name:  From the jumphost scan the keys for all infranodes  and print to file
      shell: "ssh-keyscan -H -T 10  {{ item.value.name }} >> /home/{{ adminUsername }}/.ssh/known_hosts "
      with_dict: "{{ infranodes }}"
## From the jumphost fetch the keys for all infra nodes  and save locally
    - name: From the jumphost fetch the keys for all infra nodes  and save locally
      fetch: src=/home/{{ adminUsername }}/.ssh/known_hosts  dest=/tmp/{{ item.value.name }}_knownhosts flat=yes
      with_dict: "{{ infranodes }}"

## From the jumphost scan the keys for all masters  and print to file
    - name: From the jumphost scan the keys for all masters  and print to file
      shell: "ssh-keyscan -H -T 10  {{ item.value.name }} >> /home/{{ adminUsername }}/.ssh/known_hosts "
      with_dict: "{{ masters }}"
## From the jumphost fetch the keys for all master nodes  and save locally
    - name:  From the jumphost fetch the keys for all master nodes  and save locally
      fetch: src=/home/{{ adminUsername }}/.ssh/known_hosts  dest=/tmp/{{ item.value.name }}_knownhosts flat=yes
      with_dict: "{{ masters }}"

## From the jumphost scan the keys for all jumphost  and print to file
    - name: From the jumphost scan the keys for all jumphost  and print to file
      shell: "ssh-keyscan -H -T 10  {{ item.value.name }} >> /home/{{ adminUsername }}/.ssh/known_hosts "
      with_dict: "{{ jumphost }}"
## From the jumphost fetch the keys for all jumphost nodes  and save locally
    - name:  From the jumphost fetch the keys for all jumphost nodes  and save locally
      fetch: src=/home/{{ adminUsername }}/.ssh/known_hosts  dest=/tmp/{{ item.value.name }}_knownhosts flat=yes
      with_dict: "{{ jumphost }}"

#################################################
 #### This is kinda messy, I'm appending all known hosts to local file. We fetched these in the previous step
################################################
- hosts: localhost
  connection: local
  tasks:
    - name: Assemble nodes knownhosts locally
      shell : "cat /tmp/{{ item.value.name }}_knownhosts >> ~/.ssh/known_hosts "
      with_dict: "{{ nodes }}"
    - name:  Assemble masters knownhosts locally
      shell : "cat /tmp/{{ item.value.name }}_knownhosts >> ~/.ssh/known_hosts "
      with_dict: "{{ masters }}"
    - name:  Assemble infranodes knownhosts locally
      shell : "cat /tmp/{{ item.value.name }}_knownhosts >> ~/.ssh/known_hosts "
      with_dict: "{{ infranodes }}"
    - name:  Assemble jumphost knownhosts locally
      shell : "cat /tmp/{{ item.value.name }}_knownhosts >> ~/.ssh/known_hosts "
      with_dict: "{{ jumphost }}"


## copy the ansible templated file (this is step 2 )
#################################################
 #### This may not be needed but its preparing the njumphost/master
################################################
- hosts: azure_vms
  gather_facts: False
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    routerpublicip: "{{ hostvars['localhost']['routerpublicip']}}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"

  user: "{{ adminUsername }}"
  sudo: true
  roles:
     - { role: prepare_multi }
  tasks:
     - lineinfile:
         dest: /etc/ansible/ansible.cfg
         regexp: "^#log_path"
         line: "log_path = /tmp/ansible.log"

#################################################
 #### Prepare dynamic group
#################################################
- hosts: localhost
  gather_facts: False
  connection: local
  tasks:
    - name: Add new infranodes instance to gatewayed-nodes group
      add_host: hostname={{ item.value.name }} groupname=gatewayed-nodes
      with_dict:  "{{ infranodes }}"
    - name: Add new nodes instance to gatewayed-nodes group
      add_host: hostname={{ item.value.name }} groupname=gatewayed-nodes
      with_dict:  "{{ nodes }}"
    - name: Add new instance to gatewayed-masters group
      add_host: hostname={{ item.value.name }} groupname=gatewayed-masters
      with_dict:  "{{ masters }}"

#################################################
 #### Heres the real business. Subscribing all nodes and masters Via jump host
################################################
- hosts: gatewayed-masters:gatewayed-nodes
  strategy: free
  serial: 50
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"

  user: "{{ adminUsername }}"
  sudo: true
  tasks:
      - debug : msg=" {{ ansible_ssh_common_args }} "
  roles:
    - { role: subscribe, rhnuser: "{{ rh_subcription_user }}", rhnpassword: "{{ rh_subcription_pass }}" }

#################################################
 #### Heres the real business. Preparing only master. Via jump host
 #### Prepare separate mount points for things like etcd
################################################
- hosts: gatewayed-masters
  strategy: free
  gather_facts: False
  serial: 50
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"
  user: "{{ adminUsername }}"
  sudo: true
  tasks:
  - name: Install Master stuff
    yum:
      name: "{{ item }}"
      state: present
    with_items:
      - atomic-openshift-utils
  - name: upgrade all packages
    yum: name=* state=latest
  - lineinfile:
      dest: /etc/ansible/ansible.cfg
      regexp: "^#log_path"
      line: "log_path = /tmp/ansible.log"

#################################################
 #### Heres the real business. Preparing nodes(including infra).  See prereqs_azure for details. Via jump host
################################################
- hosts: gatewayed-masters:gatewayed-nodes
  gather_facts: False
  strategy: free
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"

  user: "{{ adminUsername }}"
  sudo: true
  tasks:
    - name: creating new LVM volume group
      lvg: vg={{ create_vgname }} pvs={{ docker_storage_device }} state=present
#################################################
 #### Heres the real business. Preparing nodes(including infra).  See prereqs_azure for details. Via jump host
################################################
- hosts: gatewayed-masters:gatewayed-nodes
  gather_facts: False
  strategy: free
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"

  user: "{{ adminUsername }}"
  sudo: true
  roles:
    - { role: prereqs_azure  }
    - { role: lv_create, filesystem: 'xfs', create_lvname: 'logs-lv', create_vgname: 'docker_vg', new_mntp: '/var/log', create_lvsize: '10240' }

#################################################
 #### Heres the real business. Preparing only master. Via jump host
################################################
- hosts: gatewayed-masters
  strategy: free
  gather_facts: False
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"

  user: "{{ adminUsername }}"
  sudo: true
  ## TODO, need to check this. stopped working
  roles:
    - { role: lv_create, filesystem: 'xfs', create_lvname: 'etcd-lv', create_vgname: 'docker_vg', new_mntp: '/var/lib/origin/openshift.local.etcd/', create_lvsize: '4096' }

#################################################
 #### Heres the real business. Preparing nodes(including infra).  See prereqs_azure for details. Via jump host
################################################
- hosts: gatewayed-masters:gatewayed-nodes
  gather_facts: False
  serial: 50
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"

  user: "{{ adminUsername }}"
  sudo: true
  roles:
    - { role: storage_docker }

#################################################
 #### Heres the real business. Preparing nodes(including infra).  See prereqs_azure for details. Via jump host
################################################
#- hosts: gatewayed-nodes
#  serial: 3
#  vars:
#    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
#    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
#    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
#  user: "{{ adminUsername }}"
#  sudo: true
#  roles:
#    - { role: storage_docker }
#

################################################
 #### Installing openshift
################################################
- hosts: azure_vms
  gather_facts: False
  user: "{{ adminUsername }}"
 # sudo: true  
  force_handlers: yes
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"
  tags:
    - installopenshift
  tasks:
  - debug: msg="Beginning openshift install. This can take some time. Do not cancel this step"
  - debug: msg="To following the progress of the install, You can login using your user {{ adminUsername }}@{{ publicjumpip }} and tail the log file /tmp/ansible.logi"
  - debug: msg="After installation, the router, registry, metrics and logging are installed"
  - name: install openshift
    async: 7200
    ignore_errors: yes
    #notify: Get Ansible log files
    shell: "ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml  "
    #shell: "echo hello"
    args:
      chdir: /home/{{ adminUsername }}
  - name: "Checking output of ansible log"
#    command: curl  --insecure --silent https://{{ publicjumpdns }}:8443/healthz/ready
    command: grep 'PLAY RECAP' /tmp/ansible.log
    register: result_file
    until: result_file.stdout.find("PLAY RECAP") != -1
    retries: 400
    delay: 30
  - name: "Checking loadbalancer api"
    command: curl  --insecure --silent https://{{ publicjumpdns }}:8443/healthz/ready
    register: resultapi
    until: resultapi.stdout.find("ok") != -1
    retries: 10
    delay: 3

#- hosts: gatewayed-masters
#  name: "Check if master apis are up after install"
#  tags:
#   - postinstall
#   - masterhealth
#  vars:
#    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
#    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
#    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
#    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"
#  tasks:
#    - command: curl  --insecure --silent https://localhost:8443/healthz/ready
#      name: "Check if master apis are up"
#      register: resultmaster
#      until: resultmaster.stdout.find("ok") != -1
#      retries: 10
#      delay: 3
 

 

- hosts: gatewayed-masters
  name: restart masters after ose install
  user: "{{ adminUsername }}"
  sudo: true
  tags:
    - postinstall
    - restart
    - restart-master
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"
  tasks:
 
    - service: name=atomic-openshift-master-controllers state=restarted
    - service: name=atomic-openshift-master-api state=restarted
#    - command: curl  --insecure --silent https://localhost:8443/healthz/ready
#      register: resultmaster2
#      name: "checking master api on all masters post restart"
#      until: resultmaster2.stdout.find("ok") != -1
#      retries: 15
#      delay: 3
- hosts: gatewayed-nodes
  name: restart nodes after ose install
  user: "{{ adminUsername }}"
  sudo: true
  tags:
    - postinstall
    - restart
    - restart-nodes
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"
  tasks:
    - service: name=atomic-openshift-node  state=restarted
    - service: name=docker  state=restarted


- hosts: gatewayed-masters
  name: postinstall
  user: "{{ adminUsername }}"
  sudo: true
  tags:
    - postinstall
  vars:
    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"
    routerpublicip: "{{ hostvars['localhost']['routerpublicip']}}"
  roles: 
     - { role: postinstall }
#     - { role: metrics,run_once: true, image_version: "latest", master_url: "https://{{ publicjumpdns }}:8443", hawkular_metrics_hostname: "hawkular.apps.{{ resource_group_name }}.{{ region }}.cloudapp.azure.com", persistent: "false" }
     - { role: metrics,run_once: true, image_version: "latest", master_url: "https://{{ publicjumpdns }}:8443", hawkular_metrics_hostname: "hawkular.apps.{{ routerpublicip }}.xip.io", persistent: "false" }
#     - { role: logging,run_once: true, master_url: "https://{{ publicjumpdns }}:8443", kibana_hostname: "kibana.apps.{{ resource_group_name }}.{{ region }}.cloudapp.azure.com", image_version: "latest", public_master_url: "https://{{ publicjumpdns }}:8443" }
     - { role: logging,run_once: true, master_url: "https://{{ publicjumpdns }}:8443", kibana_hostname: "kibana.apps.{{ routerpublicip }}.xip.io", image_version: "latest", public_master_url: "https://{{ publicjumpdns }}:8443" }

#- hosts: gatewayed-masters
#  name: restart masters after postinstall setup
#  user: "{{ adminUsername }}"
#  sudo: true
#  tags:
#    - postinstall
#    - restart 
#    - restart-master
#  vars:
#    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
#    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
#    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
#    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"
#  tasks: 
#    - service: name=atomic-openshift-master-controllers state=restarted
#    - service: name=atomic-openshift-master-api state=restarted
#
#- hosts: gatewayed-nodes
#  name: restart nodes after postinstall setup
#  user: "{{ adminUsername }}"
#  sudo: true
#  tags:
#    - postinstall
#    - restart
#    - restart-nodes
#  vars:
#    publicjumpdns: "{{ hostvars['localhost']['publicjumpdns']}}"
#    publicjumpip: "{{ hostvars['localhost']['publicjumpip']}}"
#    ansible_ssh_common_args: -o ProxyCommand="ssh -W %h:%p -q {{ adminUsername }}@{{ publicjumpip }}"
#    lbvmname: "{{ hostvars['localhost']['lbvmname']}}"
#  tasks:
#    - service: name=atomic-openshift-node  state=restarted
#    - service: name=docker  state=restarted

- hosts: localhost
  connection: local 
  name: Success message
  tasks:
    - debug: msg="Congrats. Openshift should be now up and running under https://{{ publicjumpdns }}:8443/"
    - debug: msg="You can login using your user {{ adminUsername }}@{{ publicjumpip }}"
    - debug: msg="You will need to configure your dns to talk to the openshift router under  {{ routerpublicip }}"
  handlers:
    - name: Get Ansible log files
      fetch:
        src: /tmp/ansible.log
        dest: "{{ resource_group_name }}_ansible.log"
        flat: yes

### todo, get log of ansible file for
### Todo, Simplify by defiing a jump host/gateway
## Todo,  commit code
## todo, the mnt_points need to be adjuested to reflect realworld mounts on the masters
## Seperate the masters from the nodes
## todo, Print out endpoint of master and router
## todo, Perhaps setup router and stuff




## step 1 git clone
## step 2 update properties in clone file (docker storage). Best bet is to user group_vars/all  and populate that file accordingly
# - add group vars to the previous step
## step 3
#### ansible-playbook -i /etc/ansible/prepare.hosts playbooks/subscribe.yml   --user=ivan --become-method=sudo --become --private-key=~/.ssh/id_rsa
#### ansible-playbook -i /etc/ansible/prepare.hosts playbooks/configall.azure.yml   --user=ivan --become-method=sudo --become --private-key=~/.ssh/id_rsa

#- hosts: azure_vms
#  tasks:
#   - name: Copy host file to master
#     template:
#       src: /Users/imckinle/Projects/openshift/azure-ansible/set-vars.j2
#       dest: /Users/imckinle/Projects/openshift/azure-ansible/set-vars.2
#    - name: Add new instance to host group
#      add_host: hostname={{ item['ips'][0].public_ip }} groupname=azure_vms
#      with_items: azure.deployment.instances

#https://docs.ansible.com/ansible/azure_rm_deployment_module.html
